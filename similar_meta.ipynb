{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M581gEog0VG_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymysql\n",
        "import pymorphy2\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYZStxup0VHD"
      },
      "outputs": [],
      "source": [
        "# выгрузим свежий Сфинкс\n",
        "def get_connection():\n",
        "    connection = pymysql.connect(\n",
        "        host='',\n",
        "        port=\n",
        "    )\n",
        "    return connection\n",
        "\n",
        "query = '''\n",
        "select\n",
        "nativeitemid,\n",
        "title,\n",
        "duration,\n",
        "numberoscarawards,\n",
        "numberemmyawards,\n",
        "year,\n",
        "categorynames,\n",
        "countryname,\n",
        "description,\n",
        "agerating,\n",
        "kinopoiskrating,\n",
        "imdbrating,\n",
        "genretitles,\n",
        "rolenames,\n",
        "personnames,\n",
        "refbooktitles,\n",
        "refbooktypes\n",
        "from datamart\n",
        "where type = 'MOVIE' and adult != 'ADULT'\n",
        "LIMIT 500000 OPTION max_matches=500000;\n",
        "'''\n",
        "conn = get_connection()\n",
        "df = pd.read_sql(sql=query, con=conn)\n",
        "conn.close()\n",
        "df['nativeitemid'] = df['nativeitemid'].astype(int)\n",
        "df[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEI8uG1v0VHG"
      },
      "outputs": [],
      "source": [
        "df[df['nativeitemid'] == 2100180]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p1UScgt0VHG"
      },
      "source": [
        "### Подготовка таблиц\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nS8U91k0VHH"
      },
      "outputs": [],
      "source": [
        "df = df.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlQfRIce0VHI"
      },
      "outputs": [],
      "source": [
        "# разложим в столбцы refbooktitles\n",
        "ref_dict = {\n",
        "    1:   'Языки вещания',\n",
        "    2:   'Ключевые слова',\n",
        "    3:   'Сеттинги',\n",
        "    4:   'Временные периоды',\n",
        "    5:   'Настроения',\n",
        "    6:   'Стили',\n",
        "    7:   'Темы',\n",
        "    8:   'Киностудия',\n",
        "    9:   'Жанры EpgService',\n",
        "    10:  'Сериалы EpgService',\n",
        "    11:  'Категории EpgService'\n",
        "}\n",
        "\n",
        "meta_list = []\n",
        "for string, ids in df[['refbooktitles', 'refbooktypes']].values[:]:\n",
        "    meta = [''] * 11\n",
        "    if string == '': # пропускаем пустые строки\n",
        "        meta_list.append(meta)\n",
        "        continue\n",
        "\n",
        "    string = np.array(string.split(','))\n",
        "    ids = np.array([int(i) for i in ids.split(',')])\n",
        "\n",
        "    if len(string) != len(ids): # какая-то хрень, не сопрадают иногда длины 2 полей\n",
        "        min_len = min(len(string), len(ids))\n",
        "        string = string[:min_len]\n",
        "        ids = ids[:min_len]\n",
        "\n",
        "    for i in range(1, 12):\n",
        "        meta[i - 1] = ','.join(string[ids == i]) # индексируем np.array\n",
        "    meta_list.append(meta)\n",
        "\n",
        "# соединяем все это дело\n",
        "df = pd.concat((\n",
        "    df,\n",
        "    pd.DataFrame(meta_list, columns=ref_dict.values())\n",
        "), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuF6Z1SC0VHI"
      },
      "outputs": [],
      "source": [
        "# разложим в столбцы personnames\n",
        "person_types = [\n",
        "    'Актёр',\n",
        "    'Режиссёр',\n",
        "    'Сценарист',\n",
        "    'Ведущий',\n",
        "    'Композитор',\n",
        "    'Оператор',\n",
        "    'Продюсер'\n",
        "]\n",
        "\n",
        "person_list = []\n",
        "for string, ids in df[['personnames', 'rolenames']].values[:]:\n",
        "    person = [''] * 7\n",
        "    if string == '': # пропускаем пустые строки\n",
        "        person_list.append(person)\n",
        "        continue\n",
        "\n",
        "    string = np.array(string.split(','))\n",
        "    ids = np.array([s for s in ids.split(',')])\n",
        "\n",
        "    if len(string) != len(ids): # какая-то хрень, не сопрадают иногда длины 2 полей\n",
        "        min_len = min(len(string), len(ids))\n",
        "        string = string[:min_len]\n",
        "        ids = ids[:min_len]\n",
        "\n",
        "    for i in range(len(person_types)):\n",
        "        person[i] = ','.join(string[ids == person_types[i]]) # индексируем np.array\n",
        "    person_list.append(person)\n",
        "\n",
        "# соединяем все это дело\n",
        "df = pd.concat((\n",
        "    df,\n",
        "    pd.DataFrame(person_list, columns=person_types)\n",
        "), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD_FpPsI0VHI"
      },
      "outputs": [],
      "source": [
        "# нормальный формат даты\n",
        "df['year'] = pd.to_datetime(df['year'], unit='s').dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4o58a2A0VHJ"
      },
      "outputs": [],
      "source": [
        "# распрасим описание на слова, приведенные к нормальной форме\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "vectorizer = CountVectorizer(max_df=100, min_df=1)\n",
        "\n",
        "stop_words = [\n",
        "'и','в','во','не','что','он','на','я','с','со','как','а','то','все','она','так','его',\n",
        "    'но','да','ты','к','у','же','вы','за','бы','по','только','ее','мне','было','вот','от','меня','еще',\n",
        "    'нет','о','из','ему','теперь','когда','даже','ну','вдруг','ли','если','уже','или','ни','быть','был',\n",
        "    'него','до','вас','нибудь','опять','уж','вам','ведь','там','потом','себя','ничего','ей','может','они',\n",
        "    'тут','где','есть','надо','ней','для','мы','тебя','их','чем','была','сам','чтоб','без','будто','чего','раз',\n",
        "    'тоже','себе','под','будет','ж','тогда','кто','этот','того','потому','этого','какой','совсем','ним','здесь',\n",
        "    'этом','один','почти','мой','тем','чтобы','нее','сейчас','были','куда','зачем','всех','никогда','можно','при',\n",
        "    'наконец','два','об','другой','хоть','после','над','больше','тот','через','эти','нас','про','всего','них','какая',\n",
        "    'много','разве','три','эту','моя','впрочем','хорошо','свою','этой','перед','иногда','лучше','чуть','том','нельзя',\n",
        "    'такой','им','более','всегда','конечно','всю','между'\n",
        "]\n",
        "\n",
        "def get_normolize_text(text):\n",
        "    vectorizer.fit([text])\n",
        "    features = []\n",
        "\n",
        "    for f in vectorizer.get_feature_names():\n",
        "        norm_f = morph.parse(f)[0].normal_form\n",
        "        if norm_f not in stop_words:\n",
        "            features.append(norm_f)\n",
        "\n",
        "\n",
        "    return ','.join(features)\n",
        "\n",
        "df['norm_description'] = df['description'].apply(get_normolize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlO0BcyB0VHJ"
      },
      "source": [
        "### Подготовка матриц для расстояний"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvbK3aC90VHJ"
      },
      "outputs": [],
      "source": [
        "number_dict = dict(zip(\n",
        "    df.index,\n",
        "    df['nativeitemid']))\n",
        "\n",
        "asset_dict = dict(zip(\n",
        "    df['nativeitemid'],\n",
        "    df.index))\n",
        "\n",
        "name_dict = dict(zip(\n",
        "    df['nativeitemid'],\n",
        "    df['title']\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83LRv9In0VHK"
      },
      "outputs": [],
      "source": [
        "data_corpus = df['norm_description'].tolist()\n",
        "vectorizer = CountVectorizer(max_df=500, min_df=10) # это параметры для тюнинга ключевых слов\n",
        "X = vectorizer.fit_transform(data_corpus)\n",
        "\n",
        "words = vectorizer.get_feature_names()\n",
        "matrix = X.toarray()\n",
        "\n",
        "def get_key_words(index):\n",
        "    '''\n",
        "    возвращает список слов, через \",\" которые прошли фильтр CountVectorizer для конкретного фильма\n",
        "    '''\n",
        "    key_words = [w for w, v in zip(words, matrix[index]) if v != 0]\n",
        "    return key_words #','.join(key_words)\n",
        "\n",
        "key_words = list(map(get_key_words, df.index))\n",
        "df['key_words'] = list(map(lambda x: ','.join(x), key_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-jdU3iN0VHK",
        "outputId": "48fe6deb-9a59-4612-d0d5-1751015f56c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.01"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def r():\n",
        "    return round(np.random.uniform(-0.2, 0.2), 2)\n",
        "#     return 0\n",
        "r()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NupYKpeA0VHK"
      },
      "outputs": [],
      "source": [
        "################# ТЮНИНГ ########################\n",
        "features = [\n",
        "#     ('categorynames', 0.7 + r()),\n",
        "    ('countryname', 1.0 + r()),\n",
        "    ('genretitles', 1.2 + r()),\n",
        "    ('Ключевые слова', 0.5 + r()),\n",
        "    ('Сеттинги', 0.5 + r()),\n",
        "    ('Временные периоды', 0.5 + r()),\n",
        "#     ('Настроения', 1 + r()),\n",
        "    ('Стили', 0.5 + r()),\n",
        "    ('Темы', 0.5 + r()),\n",
        "    ('Киностудия', 1.5 + r()),\n",
        "    ('Категории EpgService', 0.5 + r()),\n",
        "    ('key_words', 1.5 + r()),\n",
        "    # главные актеры\n",
        "    ('Актёр', 1.5 + r()),\n",
        "    ('Режиссёр', 1.5 + r()),\n",
        "    ('Сценарист', 1.2 + r()),\n",
        "#     ('Ведущий', 2 + r()),\n",
        "    ('Композитор', 0.7 + r()),\n",
        "    ('Оператор', 0.5 + r()),\n",
        "    ('Продюсер', 0.2 + r())\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi2ZRkv00VHL"
      },
      "outputs": [],
      "source": [
        "# удалим пробелы у персон\n",
        "for f, _ in features:\n",
        "    df[f] = df[f].apply(lambda x: x.replace(' ', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2AJChI20VHL"
      },
      "outputs": [],
      "source": [
        "def get_matrix(feature, max_df=10000, min_df=1):\n",
        "    '''описать'''\n",
        "    data_corpus = df[feature]\n",
        "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df)\n",
        "    X = vectorizer.fit_transform(data_corpus).toarray()\n",
        "    F = vectorizer.get_feature_names()\n",
        "    return X, F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "t5-bL7hZ0VHL"
      },
      "outputs": [],
      "source": [
        "group_matrix = [] # матрица, сложенная из групп фичей\n",
        "feature_list = [] # список фич\n",
        "\n",
        "for feature, _ in features:\n",
        "#     print(feature)\n",
        "    add_matrix, feature_names =  get_matrix(feature)\n",
        "    group_matrix.append(add_matrix)\n",
        "\n",
        "    feature_list.extend(feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM-dwGbq0VHL",
        "outputId": "946131a3-af2a-4f46-f9e2-17230bbda98f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 52397)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_list = np.array(feature_list)\n",
        "feature_list = feature_list.reshape(1, feature_list.shape[0])\n",
        "feature_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csj16Ef10VHM"
      },
      "outputs": [],
      "source": [
        "# тут мы можем перебирать веса сколько угодно\n",
        "\n",
        "# тут нужно складывать уже sparse matrix, чтобы это работало быстрее\n",
        "matrix = np.array([[]] * df.shape[0])\n",
        "for i, m in enumerate(group_matrix):\n",
        "    matrix = np.concatenate(\n",
        "    (\n",
        "        matrix,\n",
        "        m * features[i][1] # домнажаем матрицу на вес категории\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "matrix = sp.csr_matrix(matrix) # чтобы быстро считался cos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuXfN6Yg0VHM"
      },
      "outputs": [],
      "source": [
        "# result_dict = {}\n",
        "\n",
        "# for asset_index in range(df.shape[0]):\n",
        "#     asset_id = number_dict[asset_index]\n",
        "\n",
        "#     # считаем похожих\n",
        "#     similarities = cosine_similarity(matrix, matrix[asset_index])\n",
        "#     similarities = similarities.ravel() # 2d -> 1d\n",
        "#     top = similarities.argsort()[-21:-1][::-1] # первые 20 похожих пользователей от самого релевантного\n",
        "\n",
        "#     result_dict[asset_id] = [number_dict.get(i, 0) for i in top]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPnAye_50VHM"
      },
      "outputs": [],
      "source": [
        "def asset_feature(number):\n",
        "    '''\n",
        "    возвращает список фич ассета\n",
        "    '''\n",
        "    m = matrix[number].toarray()\n",
        "    return set(feature_list[m != 0])\n",
        "\n",
        "result = []\n",
        "\n",
        "for asset_index in range(df.shape[0]):\n",
        "    asset_id = number_dict[asset_index]\n",
        "\n",
        "    # считаем похожих\n",
        "    similarities = cosine_similarity(matrix, matrix[asset_index])\n",
        "    similarities = similarities.ravel() # 2d -> 1d\n",
        "    top = similarities.argsort()[-26:][::-1] # первые 20 похожих пользователей от самого релевантного\n",
        "\n",
        "    for i in top:\n",
        "        re_assetid = number_dict[i]\n",
        "        r = [\n",
        "                asset_id, # assetid\n",
        "                re_assetid, # re_assetid\n",
        "                similarities[i], # score\n",
        "                name_dict[re_assetid], # title\n",
        "                str(asset_feature(asset_index) & asset_feature(i)) # features\n",
        "            ]\n",
        "\n",
        "        result.append(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFjpnKUr0VHM"
      },
      "source": [
        "### Обработка 4 Т"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CimpP5H-0VHM"
      },
      "outputs": [],
      "source": [
        "table_to_mysql = pd.DataFrame(result, columns=['assetid', 're_assetid', 'score', 'title', 'features'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIm43l2B0VHN"
      },
      "outputs": [],
      "source": [
        "table_to_mysql.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy_009I70VHN"
      },
      "outputs": [],
      "source": [
        "suffix = [\n",
        "    'UHD',\n",
        "    'UHD HDR',\n",
        "    '(Сурдоперевод)',\n",
        "    '(версия с тифлокомментарием)'\n",
        "]\n",
        "def title_cut(title):\n",
        "    i = max([title.find(suf) - 1 for suf in suffix])\n",
        "    if i > 0:\n",
        "        return title[:i]\n",
        "    else:\n",
        "        return title\n",
        "\n",
        "table_to_mysql['title_cut'] = table_to_mysql['title'].apply(title_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR7bTLzp0VHN"
      },
      "outputs": [],
      "source": [
        "table_to_mysql.sort_values(by=['assetid', 'score'], ascending=[False, False], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDPCgJX0VHN"
      },
      "outputs": [],
      "source": [
        "table_to_mysql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKGQUMk20VHN"
      },
      "outputs": [],
      "source": [
        "table_to_mysql.drop_duplicates(subset=['assetid', 'title_cut'], keep='first', inplace=True)\n",
        "table_to_mysql.drop('title_cut', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhvHr4e-0VHO"
      },
      "outputs": [],
      "source": [
        "table_to_mysql.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0t9jy0HJ0VHO"
      },
      "outputs": [],
      "source": [
        "table_to_mysql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wO2CfLuB0VHO"
      },
      "outputs": [],
      "source": [
        "table_to_mysql = table_to_mysql[table_to_mysql['score'].round(2) != 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB6BXlkT0VHO"
      },
      "outputs": [],
      "source": [
        "table_to_mysql.to_csv('similar_meta.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTTXbUM30VHO"
      },
      "outputs": [],
      "source": [
        "# сюда скрипт по загрузке в базу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrB356zv0VHP"
      },
      "outputs": [],
      "source": [
        "table_to_mysql.shape[0] / 12000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LhERp_n50VHP"
      },
      "outputs": [],
      "source": [
        "table_to_mysql"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}